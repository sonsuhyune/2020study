{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict 함수에서 HOC에 맞는 layer 붙이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright (c) Microsoft. All rights reserved.\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import *\n",
    "from data_utils.utils import AverageMeter\n",
    "from pytorch_pretrained_bert import BertAdam as Adam\n",
    "from module.bert_optim import Adamax, RAdam\n",
    "from mt_dnn.loss import LOSS_REGISTRY\n",
    "from .matcher import SANBertNetwork\n",
    "\n",
    "from data_utils.task_def import TaskType, EncoderModelType\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class MTDNNModel(object):\n",
    "    def __init__(self, opt, state_dict=None, num_train_step=-1):\n",
    "        self.config = opt\n",
    "        self.updates = state_dict['updates'] if state_dict and 'updates' in state_dict else 0\n",
    "        self.local_updates = 0\n",
    "        self.train_loss = AverageMeter()\n",
    "        self.network = SANBertNetwork(opt)\n",
    "        if state_dict:\n",
    "            self.network.load_state_dict(state_dict['state'], strict=False)\n",
    "        self.mnetwork = nn.DataParallel(self.network) if opt['multi_gpu_on'] else self.network\n",
    "        self.total_param = sum([p.nelement() for p in self.network.parameters() if p.requires_grad])\n",
    "        if opt['cuda']:\n",
    "            self.network.cuda()\n",
    "        optimizer_parameters = self._get_param_groups()\n",
    "        self._setup_optim(optimizer_parameters, state_dict, num_train_step)\n",
    "        self.para_swapped = False\n",
    "        self.optimizer.zero_grad()\n",
    "        self._setup_lossmap(self.config)\n",
    "\n",
    "    def _get_param_groups(self):\n",
    "        no_decay = ['bias', 'gamma', 'beta', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in self.network.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in self.network.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    def _setup_optim(self, optimizer_parameters, state_dict=None, num_train_step=-1):\n",
    "        if self.config['optimizer'] == 'sgd':\n",
    "            self.optimizer = optim.SGD(optimizer_parameters, self.config['learning_rate'],\n",
    "                                       weight_decay=self.config['weight_decay'])\n",
    "\n",
    "        elif self.config['optimizer'] == 'adamax':\n",
    "            self.optimizer = Adamax(optimizer_parameters,\n",
    "                                    self.config['learning_rate'],\n",
    "                                    warmup=self.config['warmup'],\n",
    "                                    t_total=num_train_step,\n",
    "                                    max_grad_norm=self.config['grad_clipping'],\n",
    "                                    schedule=self.config['warmup_schedule'],\n",
    "                                    weight_decay=self.config['weight_decay'])\n",
    "            if self.config.get('have_lr_scheduler', False): self.config['have_lr_scheduler'] = False\n",
    "        elif self.config['optimizer'] == 'radam':\n",
    "            self.optimizer = RAdam(optimizer_parameters,\n",
    "                                    self.config['learning_rate'],\n",
    "                                    warmup=self.config['warmup'],\n",
    "                                    t_total=num_train_step,\n",
    "                                    max_grad_norm=self.config['grad_clipping'],\n",
    "                                    schedule=self.config['warmup_schedule'],\n",
    "                                    eps=self.config['adam_eps'],\n",
    "                                    weight_decay=self.config['weight_decay'])\n",
    "            if self.config.get('have_lr_scheduler', False): self.config['have_lr_scheduler'] = False\n",
    "            # The current radam does not support FP16.\n",
    "            self.config['fp16'] = False\n",
    "        elif self.config['optimizer'] == 'adam':\n",
    "            self.optimizer = Adam(optimizer_parameters,\n",
    "                                  lr=self.config['learning_rate'],\n",
    "                                  warmup=self.config['warmup'],\n",
    "                                  t_total=num_train_step,\n",
    "                                  max_grad_norm=self.config['grad_clipping'],\n",
    "                                  schedule=self.config['warmup_schedule'],\n",
    "                                  weight_decay=self.config['weight_decay'])\n",
    "            if self.config.get('have_lr_scheduler', False): self.config['have_lr_scheduler'] = False\n",
    "        else:\n",
    "            raise RuntimeError('Unsupported optimizer: %s' % opt['optimizer'])\n",
    "\n",
    "        if state_dict and 'optimizer' in state_dict:\n",
    "            self.optimizer.load_state_dict(state_dict['optimizer'])\n",
    "\n",
    "        if self.config['fp16']:\n",
    "            try:\n",
    "                from apex import amp\n",
    "                global amp\n",
    "            except ImportError:\n",
    "                raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "            model, optimizer = amp.initialize(self.network, self.optimizer, opt_level=self.config['fp16_opt_level'])\n",
    "            self.network = model\n",
    "            self.optimizer = optimizer\n",
    "\n",
    "        if self.config.get('have_lr_scheduler', False):\n",
    "            if self.config.get('scheduler_type', 'rop') == 'rop':\n",
    "                self.scheduler = ReduceLROnPlateau(self.optimizer, mode='max', factor=self.config['lr_gamma'], patience=3)\n",
    "            elif self.config.get('scheduler_type', 'rop') == 'exp':\n",
    "                self.scheduler = ExponentialLR(self.optimizer, gamma=self.config.get('lr_gamma', 0.95))\n",
    "            else:\n",
    "                milestones = [int(step) for step in self.config.get('multi_step_lr', '10,20,30').split(',')]\n",
    "                self.scheduler = MultiStepLR(self.optimizer, milestones=milestones, gamma=self.config.get('lr_gamma'))\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "\n",
    "    def _setup_lossmap(self, config):\n",
    "        loss_types = config['loss_types']\n",
    "        self.task_loss_criterion = []\n",
    "        for idx, cs in enumerate(loss_types):\n",
    "            assert cs is not None\n",
    "            lc = LOSS_REGISTRY[cs](name='Loss func of task {}: {}'.format(idx, cs))\n",
    "            self.task_loss_criterion.append(lc)\n",
    "\n",
    "    def _setup_kd_lossmap(self, config):\n",
    "        loss_types = config['kd_loss_types']\n",
    "        self.kd_task_loss_criterion = []\n",
    "        if config.get('mkd_opt', 0) > 0:\n",
    "            for idx, cs in enumerate(loss_types):\n",
    "                assert cs is not None\n",
    "                lc = LOSS_REGISTRY[cs](name='Loss func of task {}: {}'.format(idx, cs))\n",
    "                self.kd_task_loss_criterion.append(lc)\n",
    "\n",
    "    def train(self):\n",
    "        if self.para_swapped:\n",
    "            self.para_swapped = False\n",
    "\n",
    "    def _to_cuda(self, tensor):\n",
    "        if tensor is None: return tensor\n",
    "\n",
    "        if isinstance(tensor, list) or isinstance(tensor, tuple):\n",
    "            y = [e.cuda(non_blocking=True) for e in tensor]\n",
    "            for e in y:\n",
    "                e.requires_grad = False\n",
    "        else:\n",
    "            y = tensor.cuda(non_blocking=True)\n",
    "            y.requires_grad = False\n",
    "        return y\n",
    "\n",
    "    def update(self, batch_meta, batch_data): ##########important\n",
    "        self.network.train()\n",
    "        y = batch_data[batch_meta['label']] #(batch_size, token_len)\n",
    "        soft_labels = None\n",
    "\n",
    "        task_type = batch_meta['task_type']\n",
    "        y = self._to_cuda(y) if self.config['cuda'] else y\n",
    "\n",
    "        task_id = batch_meta['task_id']\n",
    "        inputs = batch_data[:batch_meta['input_len']]\n",
    "        if len(inputs) == 3:\n",
    "            inputs.append(None)\n",
    "            inputs.append(None)\n",
    "        inputs.append(task_id)\n",
    "        weight = None\n",
    "        if self.config.get('weighted_on', False):\n",
    "            if self.config['cuda']:\n",
    "                weight = batch_data[batch_meta['factor']].cuda(non_blocking=True)\n",
    "            else:\n",
    "                weight = batch_data[batch_meta['factor']]\n",
    "        logits = self.mnetwork(*inputs) #matcher.py forward() -> logits.shape(ner) : (batch_size*token_len, label_num)\n",
    "\n",
    "        # compute loss\n",
    "        loss = 0\n",
    "        if self.task_loss_criterion[task_id] and (y is not None):\n",
    "            loss = self.task_loss_criterion[task_id](logits, y, weight, ignore_index=-1) #->한 batch의 loss \n",
    "\n",
    "        # compute kd loss\n",
    "        if self.config.get('mkd_opt', 0) > 0 and ('soft_label' in batch_meta):\n",
    "            soft_labels = batch_meta['soft_label']\n",
    "            soft_labels = self._to_cuda(soft_labels) if self.config['cuda'] else soft_labels\n",
    "            kd_lc = self.kd_task_loss_criterion[task_id]\n",
    "            kd_loss = kd_lc(logits, soft_labels, weight, ignore_index=-1) if kd_lc else 0\n",
    "            loss = loss + kd_loss\n",
    " \n",
    "        self.train_loss.update(loss.item(), batch_data[batch_meta['token_id']].size(0)) #현재 batch에 대한 loss를 저장\n",
    "        # scale loss\n",
    "        loss = loss / self.config.get('grad_accumulation_step', 1)\n",
    "        if self.config['fp16']:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        self.local_updates += 1\n",
    "        if self.local_updates % self.config.get('grad_accumulation_step', 1) == 0:\n",
    "            if self.config['global_grad_clipping'] > 0:\n",
    "                if self.config['fp16']:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer),\n",
    "                                                   self.config['global_grad_clipping'])\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.network.parameters(),\n",
    "                                                  self.config['global_grad_clipping'])\n",
    "            self.updates += 1\n",
    "            # reset number of the grad accumulation\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "####batch_meta가 뭔지 알기 위해 predict을 부르는 곳을 찾자..!!\n",
    "####batcher.py에 있다\n",
    "####\n",
    "    def predict(self, batch_meta, batch_data):\n",
    "        self.network.eval()\n",
    "        task_id = batch_meta['task_id']\n",
    "        task_type = batch_meta['task_type']\n",
    "        inputs = batch_data[:batch_meta['input_len']]\n",
    "        if len(inputs) == 3:\n",
    "            inputs.append(None)\n",
    "            inputs.append(None)\n",
    "        inputs.append(task_id)\n",
    "        score = self.mnetwork(*inputs)\n",
    "        if task_type == TaskType.Ranking:\n",
    "            score = score.contiguous().view(-1, batch_meta['pairwise_size'])\n",
    "            assert task_type == TaskType.Ranking\n",
    "            score = F.softmax(score, dim=1)\n",
    "            score = score.data.cpu()\n",
    "            score = score.numpy()\n",
    "            predict = np.zeros(score.shape, dtype=int)\n",
    "            positive = np.argmax(score, axis=1)\n",
    "            for idx, pos in enumerate(positive):\n",
    "                predict[idx, pos] = 1\n",
    "            predict = predict.reshape(-1).tolist()\n",
    "            score = score.reshape(-1).tolist()\n",
    "            return score, predict, batch_meta['true_label']\n",
    "        elif task_type == TaskType.SeqenceLabeling:\n",
    "            mask = batch_data[batch_meta['mask']]\n",
    "            score = score.contiguous() #메모리 정리 느낌(형태 같음)\n",
    "            score = score.data.cpu() #tensor를 gpu에서 cpu로 옮기기\n",
    "            score = score.numpy() #tensor를 numpy array로 바꾸기\n",
    "            predict = np.argmax(score, axis=1).reshape(mask.size()).tolist()\n",
    "            valied_length = mask.sum(1).tolist() #mask의 각 idx 값은 idx번째 sentence의 padding을 제외한 token 수(mask = [1,1,0,0,0])\n",
    "            final_predict = [] #padding 부분을 제외한 실제 token들의 BIO 태깅 예측 값들 - 정답 label과 일대일 대응\n",
    "            for idx, p in enumerate(predict):\n",
    "                final_predict.append(p[: valied_length[idx]])\n",
    "            score = score.reshape(-1).tolist()\n",
    "            return score, final_predict, batch_meta['label'] #final_predict.shape = (batch_size, 각 sentence의 token 길이)\n",
    "        elif task_type == TaskType.Span:\n",
    "            start, end = score\n",
    "            predictions = []\n",
    "            if self.config['encoder_type'] == EncoderModelType.BERT:\n",
    "                import experiments.squad.squad_utils as mrc_utils\n",
    "                scores, predictions = mrc_utils.extract_answer(batch_meta, batch_data,start, end, self.config.get('max_answer_len', 5))\n",
    "            return scores, predictions, batch_meta['answer']\n",
    "        #####################################################\n",
    "       # elif task_type== TaskType.MultiClassification:\n",
    "       #     score = score.contiguous()\n",
    "       #     score = score.data.cpu()\n",
    "       #     score = score.numpy()\n",
    "            #betch_meta\n",
    "            \n",
    "            \n",
    "        ####################################################\n",
    "            \n",
    "        else:\n",
    "            if task_type == TaskType.Classification:\n",
    "                score = F.softmax(score, dim=1)\n",
    "            score = score.data.cpu()\n",
    "            score = score.numpy()\n",
    "            predict = np.argmax(score, axis=1).tolist()\n",
    "            score = score.reshape(-1).tolist()\n",
    "        return score, predict, batch_meta['label']\n",
    "\n",
    "    def extract(self, batch_meta, batch_data):\n",
    "        self.network.eval()\n",
    "        # 'token_id': 0; 'segment_id': 1; 'mask': 2\n",
    "        inputs = batch_data[:3]\n",
    "        all_encoder_layers, pooled_output = self.mnetwork.bert(*inputs)\n",
    "        return all_encoder_layers, pooled_output\n",
    "\n",
    "    def save(self, filename):\n",
    "        network_state = dict([(k, v.cpu()) for k, v in self.network.state_dict().items()])\n",
    "        params = {\n",
    "            'state': network_state,\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'config': self.config,\n",
    "        }\n",
    "        torch.save(params, filename)\n",
    "        logger.info('model saved to {}'.format(filename))\n",
    "\n",
    "    def load(self, checkpoint):\n",
    "        model_state_dict = torch.load(checkpoint)\n",
    "        self.network.load_state_dict(model_state_dict['state'], strict=False)\n",
    "        self.optimizer.load_state_dict(model_state_dict['optimizer'])\n",
    "        self.config.update(model_state_dict['config'])\n",
    "\n",
    "    def cuda(self):\n",
    "        self.network.cuda()\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
